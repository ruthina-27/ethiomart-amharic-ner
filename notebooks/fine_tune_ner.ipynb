{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01217583",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def dataset_train_test_split(dataset, test_size=0.2, seed=42):\n",
    "    examples = [ex for ex in dataset]\n",
    "    train_examples, test_examples = train_test_split(examples, test_size=test_size, random_state=seed)\n",
    "    return Dataset.from_list(train_examples), Dataset.from_list(test_examples)\n",
    "\n",
    "train_dataset, test_dataset = dataset_train_test_split(dataset)\n",
    "\n",
    "encoded_train = train_dataset.map(tokenize_and_align_labels)\n",
    "encoded_test = test_dataset.map(tokenize_and_align_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996cc716",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlmr_model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"xlm-roberta-base\",\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "xlmr_args = TrainingArguments(\n",
    "    output_dir=\"./models/xlmr\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs/xlmr\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\"\n",
    ")\n",
    "\n",
    "xlmr_trainer = Trainer(\n",
    "    model=xlmr_model,\n",
    "    args=xlmr_args,\n",
    "    train_dataset=encoded_train,\n",
    "    eval_dataset=encoded_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForTokenClassification(tokenizer)\n",
    ")\n",
    "\n",
    "xlmr_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148715bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from seqeval.metrics import classification_report as seqeval_classification_report\n",
    "\n",
    "xlmr_preds = xlmr_trainer.predict(encoded_test)\n",
    "xlmr_pred_labels = np.argmax(xlmr_preds.predictions, axis=-1)\n",
    "xlmr_true_labels = xlmr_preds.label_ids\n",
    "\n",
    "def decode_labels(preds, true, id2label):\n",
    "    pred_tags, true_tags = [], []\n",
    "    for pred, true_seq in zip(preds, true):\n",
    "        pred_tags.append([id2label[i] for i in pred if i != -100])\n",
    "        true_tags.append([id2label[i] for i in true_seq if i != -100])\n",
    "    return pred_tags, true_tags\n",
    "\n",
    "xlmr_pred_tags, xlmr_true_tags = decode_labels(xlmr_pred_labels, xlmr_true_labels, id2label)\n",
    "\n",
    "print(\"XLM-Roberta Evaluation:\")\n",
    "print(seqeval_classification_report(xlmr_true_tags, xlmr_pred_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d259525",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "def tokenize_and_align_labels_mbert(example):\n",
    "    tokenized = mbert_tokenizer(example['tokens'], truncation=True, is_split_into_words=True)\n",
    "    word_ids = tokenized.word_ids()\n",
    "    labels = []\n",
    "    previous_word_idx = None\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            labels.append(-100)\n",
    "        elif word_idx != previous_word_idx:\n",
    "            labels.append(label2id[example[\"ner_tags\"][word_idx]])\n",
    "        else:\n",
    "            labels.append(label2id[example[\"ner_tags\"][word_idx]])\n",
    "        previous_word_idx = word_idx\n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized\n",
    "\n",
    "encoded_train_mbert = train_dataset.map(tokenize_and_align_labels_mbert)\n",
    "encoded_test_mbert = test_dataset.map(tokenize_and_align_labels_mbert)\n",
    "\n",
    "mbert_model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"bert-base-multilingual-cased\",\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "mbert_args = TrainingArguments(\n",
    "    output_dir=\"./models/mbert\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs/mbert\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\"\n",
    ")\n",
    "\n",
    "mbert_trainer = Trainer(\n",
    "    model=mbert_model,\n",
    "    args=mbert_args,\n",
    "    train_dataset=encoded_train_mbert,\n",
    "    eval_dataset=encoded_test_mbert,\n",
    "    tokenizer=mbert_tokenizer,\n",
    "    data_collator=DataCollatorForTokenClassification(mbert_tokenizer)\n",
    ")\n",
    "\n",
    "mbert_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9394d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbert_preds = mbert_trainer.predict(encoded_test_mbert)\n",
    "mbert_pred_labels = np.argmax(mbert_preds.predictions, axis=-1)\n",
    "mbert_true_labels = mbert_preds.label_ids\n",
    "\n",
    "mbert_pred_tags, mbert_true_tags = decode_labels(mbert_pred_labels, mbert_true_labels, id2label)\n",
    "\n",
    "print(\"mBERT Evaluation:\")\n",
    "print(seqeval_classification_report(mbert_true_tags, mbert_pred_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7a2938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Use a small sample for explanation\n",
    "test_sample = encoded_test.select(range(5))\n",
    "\n",
    "explainer = shap.Explainer(xlmr_model, masker=shap.maskers.Text(tokenizer))\n",
    "shap_values = explainer([tokenizer.decode(x) for x in test_sample['input_ids']])\n",
    "shap.plots.text(shap_values)\n",
    "\n",
    "# For LIME, see lime.lime_text.LimeTextExplainer (not shown here for brevity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
